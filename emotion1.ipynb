{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnTrO0RFKVn49XaTs9eT7h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prueba2001/primera-AI/blob/main/emotion1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1️⃣ Montar tu Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2️⃣ Definir la ruta del archivo ZIP en tu Drive\n",
        "# ⚠️ Ajusta la ruta según la ubicación exacta del archivo en tu Drive\n",
        "zip_path = '/content/drive/MyDrive/vit-finetuned-emotions_1.zip'\n",
        "\n",
        "# 3️⃣ Definir la carpeta donde quieres descomprimirlo\n",
        "extract_path = '/content/vit-finetuned-emotions'\n",
        "\n",
        "# 4️⃣ Descomprimir el archivo\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Crear la carpeta destino si no existe\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# Extraer el contenido\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(f\"✅ Archivo descomprimido en: {extract_path}\")"
      ],
      "metadata": {
        "id": "37CJwedkrN1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================================\n",
        "# CELDA TODO-EN-UNO: limpia paquetes conflictivos, instala dependencias compatibles,\n",
        "# y luego ejecuta el entrenamiento ViT (pegalo todo en una sola celda)\n",
        "# =================================================================================\n",
        "# --- 1) LIMPIAR (desinstalar versiones que suelen dar conflicto) ---\n",
        "!pip uninstall -y datasets transformers evaluate fsspec gcsfs accelerate >/dev/null 2>&1 || true\n",
        "\n",
        "# --- 2) Actualizar pip y luego instalar paquetes sin forzar fsspec/gcsfs ---\n",
        "!python -m pip install --upgrade pip >/dev/null\n",
        "!pip install -q --upgrade transformers datasets evaluate accelerate\n",
        "\n",
        "# --- 3) Mostrar versiones instaladas para verificar (no debería aparecer conflicto) ---\n",
        "import pkgutil, importlib, sys, subprocess, json\n",
        "def show_ver(pkg):\n",
        "    try:\n",
        "        mod = importlib.import_module(pkg)\n",
        "        v = getattr(mod, \"__version__\", \"??\")\n",
        "    except Exception:\n",
        "        v = \"no instalado\"\n",
        "    print(f\"{pkg:12s} -> {v}\")\n",
        "\n",
        "print(\"Versiones instaladas:\")\n",
        "for p in [\"pip\",\"transformers\",\"datasets\",\"evaluate\",\"accelerate\",\"fsspec\",\"gcsfs\"]:\n",
        "    if p==\"pip\":\n",
        "        import pip as _pip\n",
        "        print(f\"pip          -> {_pip.__version__}\")\n",
        "    else:\n",
        "        show_ver(p)\n",
        "\n",
        "# =================================================================================\n",
        "# Si todo ok, continúa con el código de entrenamiento (coloca aquí abajo tu código)\n",
        "# ================================================================================="
      ],
      "metadata": {
        "id": "VVgeVcpJrjLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================  CODIGO DE ENTRENAMIENTO  ==============================\n",
        "# (Ejecutar esta parte DESPUÉS de la instalación anterior en la misma celda)\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from transformers import ViTImageProcessor, ViTForImageClassification, TrainingArguments, Trainer\n",
        "import torch\n",
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "# --- RUTAS (las tuyas) ---\n",
        "train_dir = \"/content/dataset/dataset/train\"\n",
        "val_dir   = \"/content/dataset/dataset/val\"\n",
        "test_dir  = \"/content/dataset/dataset/test\"\n",
        "\n",
        "# --- CARGAR datasets (cada carpeta debe contener subcarpetas por clase) ---\n",
        "ds_train = load_dataset(\"imagefolder\", data_dir=train_dir, split=\"train\")\n",
        "ds_val   = load_dataset(\"imagefolder\", data_dir=val_dir,   split=\"train\")\n",
        "ds_test  = load_dataset(\"imagefolder\", data_dir=test_dir,  split=\"train\")\n",
        "\n",
        "ds = DatasetDict({\"train\": ds_train, \"val\": ds_val, \"test\": ds_test})\n",
        "print(\"Datasets cargados:\", ds)\n",
        "labels = ds_train.features['label'].names\n",
        "print(\"Clases encontradas:\", labels)\n",
        "if ds_val.features['label'].names != labels or ds_test.features['label'].names != labels:\n",
        "    print(\"Advertencia: las clases difieren entre splits (revisa nombres de carpetas)\")\n",
        "\n",
        "# --- PREPROCESSOR ViT ---\n",
        "model_name = \"google/vit-base-patch16-224-in21k\"\n",
        "processor = ViTImageProcessor.from_pretrained(model_name)\n",
        "\n",
        "def transformar(batch):\n",
        "    images = [img.convert(\"RGB\") for img in batch[\"image\"]]\n",
        "    inputs = processor(images, return_tensors=\"pt\")\n",
        "    # convertimos a numpy porque datasets con with_transform serializa mejor así\n",
        "    return {\"pixel_values\": inputs[\"pixel_values\"].numpy(), \"label\": batch[\"label\"]}\n",
        "\n",
        "ds_transf = ds.with_transform(transformar)\n",
        "\n",
        "# --- DATA COLLATOR ---\n",
        "def data_collator(batch):\n",
        "    # batch: lista de dicts con 'pixel_values' (numpy array) y 'label'\n",
        "    pv = torch.stack([torch.tensor(x[\"pixel_values\"]) for x in batch])\n",
        "    # si aparece dimensión extra: squeeze\n",
        "    if pv.dim() == 5 and pv.shape[1] == 1:\n",
        "        pv = pv.squeeze(1)\n",
        "    labels_tensor = torch.tensor([x[\"label\"] for x in batch], dtype=torch.long)\n",
        "    return {\"pixel_values\": pv, \"labels\": labels_tensor}\n",
        "\n",
        "# --- id2label / label2id ---\n",
        "id2label = {i: c for i, c in enumerate(labels)}\n",
        "label2id = {c: i for i, c in enumerate(labels)}\n",
        "\n",
        "# --- Modelo ---\n",
        "model = ViTForImageClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=len(labels),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "# --- Métricas ---\n",
        "acc = evaluate.load(\"accuracy\")\n",
        "f1 = evaluate.load(\"f1\")\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, refs = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    return {\n",
        "        \"accuracy\": acc.compute(predictions=preds, references=refs)[\"accuracy\"],\n",
        "        \"f1_macro\": f1.compute(predictions=preds, references=refs, average=\"macro\")[\"f1\"]\n",
        "    }\n",
        "\n",
        "# --- TrainingArguments (ajusta batch si OOM) ---\n",
        "per_device_train_bs = 8   # si OOM bajar a 4 o 2\n",
        "per_device_eval_bs = 32\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "fp16_opt = True if use_cuda else False\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./vit-finetuned-emotions\",\n",
        "    per_device_train_batch_size=per_device_train_bs,\n",
        "    per_device_eval_batch_size=per_device_eval_bs,\n",
        "    eval_strategy=\"steps\", # Renamed from evaluation_strategy\n",
        "    eval_steps=200,\n",
        "    save_steps=200,\n",
        "    logging_steps=100,\n",
        "    num_train_epochs=10,\n",
        "    learning_rate=5e-6,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1_macro\",\n",
        "    greater_is_better=True,\n",
        "    save_total_limit=3,\n",
        "    fp16=fp16_opt,\n",
        "    remove_unused_columns=False,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=ds_transf[\"train\"],\n",
        "    eval_dataset=ds_transf[\"val\"],\n",
        "    tokenizer=processor\n",
        ")\n",
        "\n",
        "print(\"Comenzando entrenamiento. GPU:\", use_cuda, \" fp16:\", fp16_opt)\n",
        "train_result = trainer.train()\n",
        "trainer.save_model()\n",
        "trainer.log_metrics(\"train\", train_result.metrics)\n",
        "trainer.save_metrics(\"train\", train_result.metrics)\n",
        "trainer.save_state()\n",
        "\n",
        "metrics_val = trainer.evaluate(eval_dataset=ds_transf[\"val\"])\n",
        "trainer.log_metrics(\"eval\", metrics_val)\n",
        "trainer.save_metrics(\"eval\", metrics_val)\n",
        "\n",
        "metrics_test = trainer.evaluate(eval_dataset=ds_transf[\"test\"])\n",
        "trainer.log_metrics(\"test\", metrics_test)\n",
        "trainer.save_metrics(\"test\", metrics_test)\n",
        "\n",
        "print(\"Validación:\", metrics_val)\n",
        "print(\"Test:\", metrics_test)\n",
        "# ================================================================================\n",
        "# FIN CELDA\n",
        "# ================================================================================"
      ],
      "metadata": {
        "id": "yu0zdNebrkcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# INFERENCIA: cargar modelo guardado, subir imagen y predecir\n",
        "import torch\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import ViTForImageClassification, ViTImageProcessor\n",
        "import numpy as np\n",
        "\n",
        "# --------- Config ----------\n",
        "model_dir = \"./vit-finetuned-emotions\"   # carpeta donde guardaste el modelo con trainer.save_model()\n",
        "model_name = \"google/vit-base-patch16-224-in21k\"  # fallback si el processor no está guardado\n",
        "top_k = 3\n",
        "# --------------------------\n",
        "\n",
        "# 1) Cargar modelo\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Dispositivo:\", device)\n",
        "model = ViTForImageClassification.from_pretrained(model_dir)\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# 2) Cargar processor (intenta desde model_dir, si no usa el model_name)\n",
        "try:\n",
        "    processor = ViTImageProcessor.from_pretrained(model_dir)\n",
        "    print(\"Processor cargado desde\", model_dir)\n",
        "except Exception as e:\n",
        "    print(\"No se pudo cargar processor desde\", model_dir, \" — usando\", model_name, \"como fallback.\")\n",
        "    processor = ViTImageProcessor.from_pretrained(model_name)\n",
        "\n",
        "# Helper para obtener label seguro (las keys en id2label a veces son str o int)\n",
        "def id2label_safe(config, idx):\n",
        "    mapping = getattr(config, \"id2label\", None)\n",
        "    if mapping is None:\n",
        "        return str(idx)\n",
        "    # Try both str and int keys\n",
        "    return mapping.get(str(idx), mapping.get(idx, str(idx)))\n",
        "\n",
        "# Función de inferencia para una imagen única\n",
        "def predecir(model, processor, img_path, top_k=3, device=device, show=True):\n",
        "    # Cargar imagen y convertir a RGB\n",
        "    image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "    # Preprocesar (el processor se encarga de resize, crop y normalización)\n",
        "    inputs = processor(images=[image], return_tensors=\"pt\")\n",
        "    # mover tensores al device\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Forward\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    logits = outputs.logits  # shape (1, num_labels)\n",
        "    probs = torch.softmax(logits, dim=-1)[0].cpu().numpy()  # vector de probabilidades\n",
        "\n",
        "    # top-k\n",
        "    topk_idx = np.argsort(probs)[::-1][:top_k]\n",
        "    results = [(int(idx), id2label_safe(model.config, int(idx)), float(probs[int(idx)])) for idx in topk_idx]\n",
        "\n",
        "    if show:\n",
        "        # Mostrar imagen y título con la mejor predicción\n",
        "        best_label = results[0][1]\n",
        "        best_prob = results[0][2]\n",
        "        plt.figure(figsize=(5,5))\n",
        "        plt.imshow(image)\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(f\"Predicción: {best_label} ({best_prob*100:.2f}%)\")\n",
        "        plt.show()\n",
        "\n",
        "        # Mostrar tabla simple de top-k\n",
        "        print(\"Top-{}:\".format(top_k))\n",
        "        for i, (idx, label, p) in enumerate(results, 1):\n",
        "            print(f\"{i}. {label} (id={idx}) — {p*100:.2f}%\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# -----------------------\n",
        "# Si estás en Colab: subir una imagen y predecir\n",
        "# -----------------------\n",
        "try:\n",
        "    # Sólo funciona en Colab / Jupyter con extensiones apropiadas\n",
        "    from google.colab import files\n",
        "    print(\"Arrastra/selecciona la(s) imagen(es) que quieres predecir (Colab Upload)...\")\n",
        "    uploaded = files.upload()\n",
        "    for fname in uploaded.keys():\n",
        "        print(\"\\n==> Prediciendo:\", fname)\n",
        "        predecir(model, processor, \"/content/\" + fname, top_k=top_k, device=device, show=True)\n",
        "\n",
        "except Exception:\n",
        "    # Fallback: si no estás en Colab, usa ruta local\n",
        "    print(\"No estás en Colab (o files.upload no disponible). Usa predecir(model, processor, '/ruta/a/tu/imagen.jpg') manualmente.\")\n",
        "    # Ejemplo de uso local:\n",
        "    # predecir(model, processor, '/content/dataset/val/fractura/0027.jpg', top_k=3)"
      ],
      "metadata": {
        "id": "NQe9TUl5segh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 1 — Comprimir carpeta a ZIP y mostrar tamaño\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "folder = \"/content/vit-finetuned-emotions\"\n",
        "output_base = \"/content/vit-finetuned-emotions\"   # salida: /content/vit-finetuned-emotions.zip\n",
        "zip_path = output_base + \".zip\"\n",
        "\n",
        "if not os.path.exists(folder):\n",
        "    raise FileNotFoundError(f\"No existe la carpeta: {folder}\")\n",
        "\n",
        "# Eliminar ZIP previo si existe\n",
        "if os.path.exists(zip_path):\n",
        "    print(\"Eliminando ZIP previo:\", zip_path)\n",
        "    os.remove(zip_path)\n",
        "\n",
        "print(\"Comprimiendo\", folder, \"->\", zip_path, \" (esto puede tardar unos segundos/minutos)...\")\n",
        "shutil.make_archive(output_base, 'zip', folder)\n",
        "size_bytes = os.path.getsize(zip_path)\n",
        "size_mb = size_bytes / 1024 / 1024\n",
        "print(f\"ZIP creado: {zip_path}  —  Tamaño: {size_mb:.2f} MB\")\n"
      ],
      "metadata": {
        "id": "PZ5L4o2qsuRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 3 — Montar Drive y copiar el ZIP a MyDrive\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "zip_path = \"/content/vit-finetuned-emotions.zip\"\n",
        "if not os.path.exists(zip_path):\n",
        "    raise FileNotFoundError(\"Ejecuta antes la celda que crea el ZIP.\")\n",
        "\n",
        "drive.mount('/content/drive')   # te pedirá autorización\n",
        "\n",
        "# Cambia el destino si quieres otra carpeta dentro de tu Drive\n",
        "dest = \"/content/drive/MyDrive/vit-finetuned-emotions.zip\"\n",
        "\n",
        "# Si existe, añadir sufijo para no sobrescribir\n",
        "if os.path.exists(dest):\n",
        "    base, ext = os.path.splitext(dest)\n",
        "    i = 1\n",
        "    while os.path.exists(f\"{base}_{i}{ext}\"):\n",
        "        i += 1\n",
        "    dest = f\"{base}_{i}{ext}\"\n",
        "\n",
        "print(\"Copiando ZIP a Drive:\", dest)\n",
        "shutil.copy(zip_path, dest)\n",
        "print(\"Copia completada. Archivo en tu Google Drive:\", dest)\n"
      ],
      "metadata": {
        "id": "LqKa7p5Asu_D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}